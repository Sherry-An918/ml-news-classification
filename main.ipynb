{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load and preprocess the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "data_path = '4.csv'\n",
        "new_df = pd.read_csv(data_path)\n",
        "\n",
        "# Fill missing values with empty strings\n",
        "new_df = new_df.fillna('')\n",
        "\n",
        "# Text preprocessing\n",
        "new_df['text'] = new_df['short_description'].apply(lambda x: re.sub(r'\\W', ' ', str(x)).lower().strip())\n",
        "\n",
        "\n",
        "new_df['label'] = pd.factorize(new_df['category'])[0]\n",
        "\n",
        "# Data set partitioning\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_df, test_df = train_test_split(new_df, test_size=0.2, random_state=42, stratify=new_df['category'])\n",
        "train_df, valid_df = train_test_split(train_df, test_size=0.25, random_state=42, stratify=train_df['category'])\n",
        "\n",
        "\n",
        "print(\"Training set size:\", len(train_df))\n",
        "print(\"Validation set size:\", len(valid_df))\n",
        "print(\"Test set size:\", len(test_df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Perform an exploration of the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Perform an analysis of the most common terms for each category\n",
        "def get_common_terms(text_series, top_n=10):\n",
        "    all_words = ' '.join(text_series).split()\n",
        "    counter = Counter(all_words)\n",
        "    return counter.most_common(top_n)\n",
        "\n",
        "\n",
        "categories = new_df['category'].unique()\n",
        "for category in categories:\n",
        "    common_terms = get_common_terms(new_df[new_df['category'] == category]['text'])\n",
        "    terms, counts = zip(*common_terms)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.barplot(x=list(counts), y=list(terms))\n",
        "    plt.title(f'Most Common Terms in Category: {category}')\n",
        "    plt.xlabel('Count')\n",
        "    plt.ylabel('Terms')\n",
        "    plt.show()\n",
        "\n",
        "# Perform an analysis of the sentence length for each category\n",
        "new_df['text_length'] = new_df['text'].apply(lambda x: len(x.split()))\n",
        "length_stats = new_df.groupby('category')['text_length'].describe()\n",
        "print(length_stats)\n",
        "\n",
        "#check the blank values\n",
        "missing_values = new_df.isnull().sum()\n",
        "print(\"Missing values in each column:\\n\", missing_values)\n",
        "\n",
        "#check the outliers\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.boxplot(x='category', y='text_length', data=new_df)\n",
        "plt.xlabel('Category')\n",
        "plt.ylabel('Text Length')\n",
        "plt.title('Boxplot of Text Length by Category')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task1.3 Perform an exploration of the data.\n",
        "\n",
        "#### Perform an analysis of the most common terms for each category. \n",
        "\n",
        "**Result**：\n",
        "- **TRAVEL**： \"the\" (over 8000 times), \"to\", \"of\", \"a\", \"and\", \"in\", \"you\", \"s\", \"is\", \"for\"。\n",
        "- **WEIRD NEWS** \"the\" (over 800 times), \"a\", \"to\", \"s\", \"it\", \"of\", \"in\", \"and\", \"is\", \"you\"。\n",
        "\n",
        "We can see that the \"the\" term is the most common term in the two categories, and these common terms are mainly common words in English, which have little impact on the construction of the classification model and need to be removed during pre-processing.\n",
        "\n",
        "#### The length of sentences in each category\n",
        "\n",
        "**Result**：\n",
        "- **TRAVEL**：The average sentence length is about 27.36 words, the standard deviation is 13.76, and the distribution of sentence lengths are relatively uniform.\n",
        "- **WEIRD NEWS**：The average sentence length is about 8.60 words, the standard deviation is 7.36 and the sentences are short and concentrated.\n",
        "\n",
        "Sentences in the TRAVEL category were widely distributed, with a maximum length of 166 words, while sentences in the WEIRD NEWS category were relatively short, with a maximum length of 50 words. It is observed that the difference in sentence length might have an effect on the classification task.\n",
        "\n",
        "There are no blank values in the dataset. The integrity of the data is good, so there is no need to fill in additional missing values and can be used directly for subsequent analysis.\n",
        "\n",
        "The box plot shows the distribution of sentence length. The sentence length of the TRAVEL category is widely distributed, and there are some sentences with longer length in the TRAVEL category, while the sentence length of the WEIRD NEWS category is mainly concentrated in 0-50 words, which might affect classification.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 2. Data Preparation & Modelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Splitting the dataset into training, development and test sets\n",
        "train_df, test_df = train_test_split(new_df, test_size=0.2, random_state=42, stratify=new_df['category'])\n",
        "train_df, valid_df = train_test_split(train_df, test_size=0.25, random_state=42, stratify=train_df['category'])\n",
        "\n",
        "\n",
        "train_df.to_csv('train.csv', index=False)\n",
        "valid_df.to_csv('valid.csv', index=False)\n",
        "test_df.to_csv('test.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        " The data set is split into training, development and test sets in a ratio of 60:20:20. Since we are dealing with an imbalance dataset where the ratio of TRAVEL to WEIRD NEWS is 3:1, the 'stratify' parameter is used to ensure that each class is equally distributed in the training set, development set, and test set, which makes the distribution of classes consistent across the splits."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " Load train.csv valid.csv files. Apply preprocessing steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "#  Load train.csv and valid.csv files. \n",
        "train_df = pd.read_csv('train.csv')\n",
        "valid_df = pd.read_csv('valid.csv')\n",
        "\n",
        "# Define text preprocessing functions\n",
        "def preprocess_text(text):\n",
        "    if isinstance(text, str):\n",
        "        text = text.lower() \n",
        "        text = re.sub(r'\\b\\w{1,2}\\b', '', text) \n",
        "        text = re.sub(r'[^\\w\\s]', '', text) \n",
        "        text = re.sub(r'\\s+', ' ', text)  \n",
        "    else:\n",
        "        text = \"\"\n",
        "    return text\n",
        "\n",
        "# Apply the preprocess function to the text column\n",
        "train_df['text'] = train_df['text'].apply(preprocess_text)\n",
        "valid_df['text'] = valid_df['text'].apply(preprocess_text)\n",
        "\n",
        "# Handle NaN values\n",
        "train_df['text'].fillna('', inplace=True)\n",
        "valid_df['text'].fillna('', inplace=True)\n",
        "\n",
        "# Use TF-IDF for text representation\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train = vectorizer.fit_transform(train_df['text'])\n",
        "X_valid = vectorizer.transform(valid_df['text'])\n",
        "\n",
        "y_train = train_df['label']\n",
        "y_valid = valid_df['label']\n",
        "\n",
        "\n",
        "print(\"Training set text representation shape:\", X_train.shape)\n",
        "print(\"Validation set text representation shape:\", X_valid.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The text was converted to lower case, punctuation and extra spaces are removed as this was noticed when I was examining the data. The text is then represented as feature vectors using TF-IDF, with a maximum of 5000 features. Finally, the TF-IDF representation of the training set and verification set is obtained. The training set has dimensions (4800, 5000), while the verification set has dimensions (1600, 5000)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import joblib\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Load the previously saved training set and validation set \n",
        "train_df = pd.read_csv('train.csv')\n",
        "valid_df = pd.read_csv('valid.csv')\n",
        "\n",
        "# Define text preprocessing functions\n",
        "def preprocess_text(text):\n",
        "    if isinstance(text, str):\n",
        "        text = text.lower()  \n",
        "        text = re.sub(r'\\b\\w{1,2}\\b', '', text) \n",
        "        text = re.sub(r'[^\\w\\s]', '', text)  \n",
        "        text = re.sub(r'\\s+', ' ', text)  \n",
        "    else:\n",
        "        text = \"\"\n",
        "    return text\n",
        "\n",
        "# Apply the function\n",
        "train_df['text'] = train_df['text'].apply(preprocess_text)\n",
        "valid_df['text'] = valid_df['text'].apply(preprocess_text)\n",
        "\n",
        "# Handle Nan\n",
        "train_df['text'].fillna('', inplace=True)\n",
        "valid_df['text'].fillna('', inplace=True)\n",
        "\n",
        "# Use TF-IDF for text representation\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train = vectorizer.fit_transform(train_df['text'])\n",
        "X_valid = vectorizer.transform(valid_df['text'])\n",
        "\n",
        "y_train = train_df['label']\n",
        "y_valid = valid_df['label']\n",
        "\n",
        "# Train a random forest model\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Save the random forest model\n",
        "joblib.dump(rf_model, 'rf_model.pkl')\n",
        "\n",
        "# Training naive Bayes models\n",
        "nb_model = MultinomialNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "\n",
        "# Save the naive Bayes model\n",
        "joblib.dump(nb_model, 'nb_model.pkl')\n",
        "\n",
        "# Load the model\n",
        "rf_model_loaded = joblib.load('rf_model.pkl')\n",
        "nb_model_loaded = joblib.load('nb_model.pkl')\n",
        "\n",
        "# Make predictions on validation sets\n",
        "rf_valid_preds = rf_model_loaded.predict(X_valid)\n",
        "nb_valid_preds = nb_model_loaded.predict(X_valid)\n",
        "\n",
        "\n",
        "rf_valid_accuracy = accuracy_score(y_valid, rf_valid_preds)\n",
        "nb_valid_accuracy = accuracy_score(y_valid, nb_valid_preds)\n",
        "\n",
        "print(\"Random Forest - Valid Accuracy:\", rf_valid_accuracy)\n",
        "print(classification_report(y_valid, rf_valid_preds, target_names=['TRAVEL', 'WEIRD NEWS', 'OTHER']))\n",
        "\n",
        "print(\"Naive Bayes - Valid Accuracy:\", nb_valid_accuracy)\n",
        "print(classification_report(y_valid, nb_valid_preds, target_names=['TRAVEL', 'WEIRD NEWS', 'OTHER']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define the data set class\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.texts[idx], self.labels[idx]\n",
        "\n",
        "# Convert data to tensors\n",
        "X_train_tensor = torch.tensor(X_train.toarray(), dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
        "X_valid_tensor = torch.tensor(X_valid.toarray(), dtype=torch.float32)\n",
        "y_valid_tensor = torch.tensor(y_valid.values, dtype=torch.long)\n",
        "\n",
        "\n",
        "train_dataset = TextDataset(X_train_tensor, y_train_tensor)\n",
        "valid_dataset = TextDataset(X_valid_tensor, y_valid_tensor)\n",
        "\n",
        "# load the data\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# CNN model\n",
        "class SimpleCNNTextClassificationModel(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(SimpleCNNTextClassificationModel, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=50, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool1d(kernel_size=2)\n",
        "        self.fc1 = nn.Linear(50 * (input_dim // 2), 128)\n",
        "        self.fc2 = nn.Linear(128, 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.unsqueeze(1)\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = x.view(x.size(0), -1) \n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "input_dim = X_train.shape[1]\n",
        "model = SimpleCNNTextClassificationModel(input_dim)\n",
        "\n",
        "# Loss functions and optimizers\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the model\n",
        "num_epochs = 10\n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "train_accuracies = []\n",
        "valid_accuracies = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for inputs, labels in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\"):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    epoch_accuracy = correct / total\n",
        "    train_losses.append(epoch_loss)\n",
        "    train_accuracies.append(epoch_accuracy)\n",
        "    \n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(valid_loader, desc=f\"Validating Epoch {epoch+1}/{num_epochs}\"):\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    epoch_loss = running_loss / len(valid_loader.dataset)\n",
        "    epoch_accuracy = correct / total\n",
        "    valid_losses.append(epoch_loss)\n",
        "    valid_accuracies.append(epoch_accuracy)\n",
        "    \n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_losses[-1]:.4f}, Train Accuracy: {train_accuracies[-1]:.4f}, Valid Loss: {valid_losses[-1]:.4f}, Valid Accuracy: {valid_accuracies[-1]:.4f}\")\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), 'simple_cnn_text_classification_model.pth')\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(1, num_epochs+1), train_losses, label='Train Loss')\n",
        "plt.plot(range(1, num_epochs+1), valid_losses, label='Valid Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(1, num_epochs+1), train_accuracies, label='Train Accuracy')\n",
        "plt.plot(range(1, num_epochs+1), valid_accuracies, label='Valid Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "As training progresses, the model learns and improves. The training and validation sets are also getting more accurate, showing that the model is improving. At epoch 10, the training set loss is 0.2622 and the accuracy is 0.8900; the validation set loss is 0.3164 and the accuracy is 0.8562.\n",
        "The overall trend is upward, and the final verification accuracy is close to the training accuracy. This shows that the model performs well on both the training set and the verification set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " ### Task3: Evaluate model performance on training and validation sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Load the model\n",
        "model.load_state_dict(torch.load('simple_cnn_text_classification_model.pth'))\n",
        "\n",
        "# Evaluate model performance on validation sets\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "valid_preds = []\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in valid_loader:\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        valid_preds.extend(predicted.cpu().numpy())\n",
        "valid_accuracy = correct / total\n",
        "print(f\"Validation Accuracy: {valid_accuracy:.4f}\")\n",
        "\n",
        "\n",
        "y_valid_pred = np.array(valid_preds)\n",
        "print(classification_report(y_valid_tensor.numpy(), y_valid_pred, target_names=['TRAVEL', 'WEIRD NEWS', 'OTHER']))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "error_analysis_df = pd.DataFrame({\n",
        "    'text': valid_df['text'].values,\n",
        "    'true_label': y_valid_tensor.numpy(),\n",
        "    'predicted_label': y_valid_pred\n",
        "})\n",
        "\n",
        "# The misclassified samples were filtered out\n",
        "error_analysis_df = error_analysis_df[error_analysis_df['true_label'] != error_analysis_df['predicted_label']]\n",
        "\n",
        "\n",
        "print(error_analysis_df.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Many TRAVEL texts are misclassified as WEIRD NEWS, probably because the two categories share similarities in some features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import joblib\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "target_names = ['TRAVEL', 'WEIRD NEWS', 'OTHER']\n",
        "\n",
        "# Random forest model optimization: Increase the number of trees\n",
        "optimized_rf_model = RandomForestClassifier(n_estimators=200, random_state=42)\n",
        "optimized_rf_model.fit(X_train, y_train)\n",
        "optimized_rf_valid_preds = optimized_rf_model.predict(X_valid)\n",
        "optimized_rf_valid_accuracy = accuracy_score(y_valid, optimized_rf_valid_preds)\n",
        "print(\"Optimized Random Forest - Valid Accuracy:\", optimized_rf_valid_accuracy)\n",
        "print(classification_report(y_valid, optimized_rf_valid_preds, target_names=target_names, zero_division=0))\n",
        "\n",
        "# Save it\n",
        "joblib.dump(optimized_rf_model, 'optimized_rf_model.pkl')\n",
        "\n",
        "# Naive Bayesian model optimization: using smoothing parameters\n",
        "optimized_nb_model = MultinomialNB(alpha=0.5)\n",
        "optimized_nb_model.fit(X_train, y_train)\n",
        "optimized_nb_valid_preds = optimized_nb_model.predict(X_valid)\n",
        "optimized_nb_valid_accuracy = accuracy_score(y_valid, optimized_nb_valid_preds)\n",
        "print(\"Optimized Naive Bayes - Valid Accuracy:\", optimized_nb_valid_accuracy)\n",
        "print(classification_report(y_valid, optimized_nb_valid_preds, target_names=target_names, zero_division=0))\n",
        "\n",
        "# Save the optimized naive Bayes model\n",
        "joblib.dump(optimized_nb_model, 'optimized_nb_model.pkl')\n",
        "\n",
        "# Deep learning model optimization: Change the model structure and increase the training rounds\n",
        "class SimpleCNNModel(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(SimpleCNNModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 256)\n",
        "        self.dropout1 = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc3 = nn.Linear(128, 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "optimized_model = SimpleCNNModel(input_dim)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(optimized_model.parameters(), lr=0.001)\n",
        "\n",
        "# Retrain the deep learning model\n",
        "num_epochs = 10\n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "train_accuracies = []\n",
        "valid_accuracies = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    optimized_model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for inputs, labels in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}/{num_epochs}\"):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = optimized_model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    epoch_accuracy = correct / total\n",
        "    train_losses.append(epoch_loss)\n",
        "    train_accuracies.append(epoch_accuracy)\n",
        "    \n",
        "    optimized_model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(valid_loader, desc=f\"Validating Epoch {epoch+1}/{num_epochs}\"):\n",
        "            outputs = optimized_model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    epoch_loss = running_loss / len(valid_loader.dataset)\n",
        "    epoch_accuracy = correct / total\n",
        "    valid_losses.append(epoch_loss)\n",
        "    valid_accuracies.append(epoch_accuracy)\n",
        "    \n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_losses[-1]:.4f}, Train Accuracy: {train_accuracies[-1]:.4f}, Valid Loss: {valid_losses[-1]:.4f}, Valid Accuracy: {valid_accuracies[-1]:.4f}\")\n",
        "\n",
        "# Save it\n",
        "torch.save(optimized_model.state_dict(), 'optimized_cnn_model.pth')\n",
        "\n",
        "# Visual training process\n",
        "plt.figure(figsize=(14, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(1, num_epochs+1), train_losses, label='Train Loss')\n",
        "plt.plot(range(1, num_epochs+1), valid_losses, label='Valid Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(1, num_epochs+1), train_accuracies, label='Train Accuracy')\n",
        "plt.plot(range(1, num_epochs+1), valid_accuracies, label='Valid Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Optimized Random Forest:\n",
        "**Valid Accuracy: 0.865625**\n",
        "By increasing the number of trees, the performance of the random forest model on the verification set is improved. The TRAVEL category had the highest accuracy, and the WEIRD NEWS category also improved\n",
        "#### Optimized Naive Bayes:\n",
        "**Valid Accuracy: 0.78375**\n",
        "By adjusting the smoothing parameter alpha, the performance of the model is improved, and the accuracy on the verification set is improved. Accuracy was higher in the TRAVEL category, but the WEIRD NEWS category still needed improvement.\n",
        "#### Optimized CNN Model:\n",
        "The accuracy and loss curves during training show how the model performs on the training set and the validation set. By simplifying the model structure and increasing the number of training rounds, the performance of the model on the verification set is further improved. The accuracy of the validation set is 0.8606, indicating that the model performance has improved."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Combine the training set and the validation set for cross-validation\n",
        "X_combined = np.vstack((X_train.toarray(), X_valid.toarray()))\n",
        "y_combined = np.concatenate((y_train, y_valid))\n",
        "\n",
        "# Cross-validate random forest models\n",
        "optimized_rf_model = joblib.load('optimized_rf_model.pkl')\n",
        "rf_cv_scores = cross_val_score(optimized_rf_model, X_combined, y_combined, cv=5, scoring='accuracy')\n",
        "print(\"Random Forest Cross-Validation Accuracy:\", rf_cv_scores)\n",
        "print(\"Mean CV Accuracy:\", np.mean(rf_cv_scores))\n",
        "\n",
        "# Cross-validate naive Bayes model\n",
        "optimized_nb_model = joblib.load('optimized_nb_model.pkl')\n",
        "nb_cv_scores = cross_val_score(optimized_nb_model, X_combined, y_combined, cv=5, scoring='accuracy')\n",
        "print(\"Naive Bayes Cross-Validation Accuracy:\", nb_cv_scores)\n",
        "print(\"Mean CV Accuracy:\", np.mean(nb_cv_scores))\n",
        "\n",
        "class SimpleCNNModel(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(SimpleCNNModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 256)\n",
        "        self.dropout1 = nn.Dropout(0.5)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc3 = nn.Linear(128, 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Load the optimized CNN model\n",
        "input_dim = X_train.shape[1]\n",
        "optimized_model = SimpleCNNModel(input_dim)\n",
        "optimized_model.load_state_dict(torch.load('optimized_cnn_model.pth'))\n",
        "\n",
        "# Use KFold for cross-validation\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "kf = KFold(n_splits=5)\n",
        "cnn_cv_scores = []\n",
        "\n",
        "for train_index, val_index in kf.split(X_combined):\n",
        "    X_train_kf, X_val_kf = X_combined[train_index], X_combined[val_index]\n",
        "    y_train_kf, y_val_kf = y_combined[train_index], y_combined[val_index]\n",
        "    \n",
        "    train_dataset_kf = TensorDataset(torch.tensor(X_train_kf, dtype=torch.float32), torch.tensor(y_train_kf, dtype=torch.long))\n",
        "    val_dataset_kf = TensorDataset(torch.tensor(X_val_kf, dtype=torch.float32), torch.tensor(y_val_kf, dtype=torch.long))\n",
        "    \n",
        "    train_loader_kf = DataLoader(train_dataset_kf, batch_size=64, shuffle=True)\n",
        "    val_loader_kf = DataLoader(val_dataset_kf, batch_size=64, shuffle=False)\n",
        "    \n",
        "    model = SimpleCNNModel(input_dim)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    \n",
        "    for epoch in range(5):  \n",
        "        model.train()\n",
        "        for inputs, labels in train_loader_kf:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "    \n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader_kf:\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    \n",
        "    accuracy = correct / total\n",
        "    cnn_cv_scores.append(accuracy)\n",
        "\n",
        "print(\"CNN Cross-Validation Accuracy:\", cnn_cv_scores)\n",
        "print(\"Mean CV Accuracy:\", np.mean(cnn_cv_scores))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Based on the cross-validation results, the CNN model is the best performing model, followed by the Random forest model. The highest average accuracy of the CNN model in the task indicates that it is the most suitable model for this text classification task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load test set\n",
        "test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# Preprocessing test set text\n",
        "test_df['text'] = test_df['text'].apply(preprocess_text)\n",
        "X_test = vectorizer.transform(test_df['text'])\n",
        "y_test = test_df['label']\n",
        "\n",
        "# Load the optimized CNN model\n",
        "optimized_model = SimpleCNNModel(input_dim)\n",
        "optimized_model.load_state_dict(torch.load('optimized_cnn_model.pth'))\n",
        "\n",
        "\n",
        "test_dataset = TensorDataset(torch.tensor(X_test.toarray(), dtype=torch.float32), torch.tensor(y_test, dtype=torch.long))\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Evaluate the optimized CNN model on the test set\n",
        "optimized_model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        outputs = optimized_model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "test_accuracy = correct / total\n",
        "print(\"Optimized CNN - Test Accuracy:\", test_accuracy)\n",
        "\n",
        "\n",
        "y_test_pred = []\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        outputs = optimized_model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        y_test_pred.extend(predicted.numpy())\n",
        "\n",
        "print(classification_report(y_test, y_test_pred, target_names=target_names))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "The accuracy of the optimized CNN model on the test set is 0.835, which is very close to the average accuracy of the cross-validation of 0.8573. It shows that the model has strong generalization ability on unseen data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "Retrain the model and evaluate the test set using the combined training and validation sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import joblib\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "# Redefine the CNN model\n",
        "class SimpleCNNModel(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(SimpleCNNModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Load the training set and the validation set\n",
        "train_df = pd.read_csv('train.csv')\n",
        "valid_df = pd.read_csv('valid.csv')\n",
        "test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# Combine them\n",
        "combined_df = pd.concat([train_df, valid_df])\n",
        "combined_df['text'].fillna('', inplace=True)\n",
        "\n",
        "# Use TF-IDF for text representation\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_combined = vectorizer.fit_transform(combined_df['text'])\n",
        "y_combined = combined_df['label']\n",
        "\n",
        "# Convert to tensor\n",
        "X_combined_tensor = torch.tensor(X_combined.toarray(), dtype=torch.float32)\n",
        "y_combined_tensor = torch.tensor(y_combined.values, dtype=torch.long)\n",
        "\n",
        "# Data loader\n",
        "combined_dataset = TensorDataset(X_combined_tensor, y_combined_tensor)\n",
        "combined_loader = DataLoader(combined_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Define the model, loss function, and optimizer\n",
        "input_dim = X_combined.shape[1]\n",
        "retrained_model = SimpleCNNModel(input_dim)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(retrained_model.parameters(), lr=0.001)\n",
        "\n",
        "# Training model\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    retrained_model.train()\n",
        "    for inputs, labels in tqdm(combined_loader, desc=f'Training Epoch {epoch+1}/{num_epochs}'):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = retrained_model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Evaluate the retrained CNN model on the test set\n",
        "test_df['text'].fillna('', inplace=True)\n",
        "X_test = vectorizer.transform(test_df['text'])\n",
        "y_test = test_df['label']\n",
        "\n",
        "X_test_tensor = torch.tensor(X_test.toarray(), dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
        "\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "retrained_model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "y_test_pred = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        outputs = retrained_model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        y_test_pred.extend(predicted.numpy())\n",
        "\n",
        "test_accuracy_retrained = correct / total\n",
        "print(\"Retrained CNN - Test Accuracy:\", test_accuracy_retrained)\n",
        "\n",
        "\n",
        "print(classification_report(y_test, y_test_pred, target_names=['TRAVEL', 'WEIRD NEWS', 'OTHER']))\n",
        "\n",
        "torch.save(retrained_model.state_dict(), 'retrained_cnn_model.pth')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "**The retrained CNN model performed well with an overall accuracy of 0.84 on the test set.**\n",
        "For **TRAVEL**, the model performed well, with accuracy, recall and F1 scores of 0.89.\n",
        "**WEIRD NEWS** reports that both the accuracy rate and recall rate of the model are 0.68\n",
        "OTHER, due to the small sample size (only 4), the model did not successfully predict this category, resulting in accuracy, recall, and F1 scores of 0.00"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (Pyodide)",
      "language": "python",
      "name": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "c187e67ceb5f6e4e3595028940f5d6d11d71f78ddf764c8fba82f6ce735cf85f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
